<h1 style="text-align: center;">VeRL-with-RMs: ä½¿ç”¨å¥–åŠ±æ¨¡å‹æ›´å¥½åœ°æŒ‡å¯¼ Policy è®­ç»ƒ </h1>

ğŸŒŸ å…³é”®ç‰¹æ€§ï¼š
æ”¯æŒå¤šç§å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚åŸºäº vllm åç«¯å®ç°é«˜æ•ˆçš„ RM æ‰“åˆ†ï¼ŒåŒæ—¶é€šè¿‡åŠ¨æ€è£…å¸è½½ RM åŠå„ç»„ä»¶ï¼Œä¼˜åŒ–æ˜¾å­˜åˆ©ç”¨ã€‚æ”¯æŒ RM ä¸è§„åˆ™çš„æ··åˆæ‰“åˆ†æ¨¡å¼ã€‚æ”¯æŒå¤š RM å®ä¾‹ååŒæ‰“åˆ†æ¨¡å¼ã€‚


## ä½¿ç”¨æ–¹æ³•ï¼ˆæš‚æ”¯æŒåˆ°GRMï¼‰ï¼š

1. åœ¨æ ‡å‡†çš„åŸºäºè§„åˆ™çš„ RL è®­ç»ƒæµç¨‹æ‰€éœ€è¦å‡†å¤‡çš„ç»„ä»¶åŸºç¡€ä¸Šï¼Œå‡†å¤‡ä¸€ä¸ª RM æˆ– off-the-shelf LLM ä½œä¸ºæ‰“åˆ†æ¨¡å‹ã€‚
2. å‚è€ƒ `verl/verl/trainer/config/ppo_trainer.yaml` ä¸­ `reward_model` é¡¹ä¸‹å®šä¹‰çš„ç›¸å…³å‚æ•°ï¼Œåœ¨è¿è¡Œ `verl.trainer.main_ppo` é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ä¼ å…¥ä»¥æ§åˆ¶æ¨¡å‹è¡Œä¸ºã€‚æ¯”å¦‚ `reward_model.model.path` æ§åˆ¶è¯»å–çš„ RM çš„è·¯å¾„ï¼Œ`reward_model.reward_model_type` æ§åˆ¶ RM æ‰“åˆ†ç±»å‹ç­‰ç­‰ã€‚
3. æ•°æ®å¤„ç†æ ¼å¼è¦æ±‚ï¼šéœ€è¦åœ¨æ¨¡å‹è¯»å–çš„ parquet æ–‡ä»¶ä¸­å®šä¹‰å¦‚ä¸‹æ•°æ®é¡¹ï¼Œ`prompt`ï¼ˆpolicy model çš„ querysï¼‰ï¼Œ`rm_prompt`ï¼ˆreward model çš„ promptsï¼Œä¹Ÿå¯ä»¥æ˜¯ç©ºstrï¼Œåœ¨åç»­ `reward_function` ä¸­å†å®šä¹‰ï¼‰ï¼Œ`reward_model`ï¼ˆå‚è€ƒç­”æ¡ˆï¼‰ã€‚æ•°æ®å¤„ç†æ–‡ä»¶å¯å‚è€ƒ `verl/examples/data_preprocess/process_rider.py`ã€‚
4. Reward å‰åå¤„ç†ï¼š

   (1) åœ¨ `verl/workers/reward_function` æ–‡ä»¶å¤¹ä¸‹å®šä¹‰ç”¨äº Reward è®¡ç®—å‰å¤„ç†ä¸åå¤„ç†æ–¹æ³•çš„ `.py` æ–‡ä»¶ï¼Œå¹¶ä¸”éœ€è¦ç¡®ä¿ config çš„ `reward_model.reward_function` çš„å€¼ä¸æ–‡ä»¶åç›¸åŒä»è€Œä½¿æ–¹æ³•èƒ½å¤Ÿæ­£ç¡®æ³¨å†Œã€‚æ¯”å¦‚ï¼Œåœ¨ `rider_reward.py` é‡Œå®šä¹‰äº†ç”¨äºéª‘æ‰‹ RL è®­ç»ƒçš„ reward å¤„ç†è§„åˆ™ï¼Œå°±è¦åœ¨ config æˆ–å‘½ä»¤è¡Œä¼ å‚ä¸­å°† `reward_model.reward_function` çš„å€¼èµ‹ä¸º `rider_reward`ã€‚
   
   (2) æ¯ä¸ª `reward_function` æ–‡ä»¶ä¸­éƒ½éœ€è¦å®ç°ä¸¤ä¸ªå‡½æ•° `rm_preprocess()` ä»¥åŠ `rm_postprocess()`ï¼Œ`rm_preprocess()` è´Ÿè´£åˆ¶ä½œ RM çš„è¾“å…¥ï¼Œéœ€ä¼ å…¥ `text_prompts`ï¼ˆRM çš„promptï¼‰ï¼Œ`text_responses`ï¼ˆPolicy çš„rolloutæ–‡æœ¬ï¼‰ï¼Œ`tgt_tokenizer`ï¼ˆRM çš„ tokenizerï¼‰ï¼Œä¼ å‡ºè¦è¾“å…¥ç»™ RM çš„æ–‡æœ¬ã€‚`rm_postprocess()` éœ€è¦å¯¹ RM è¾“å‡ºå†…å®¹è¿›è¡Œåå¤„ç†æå–å‡ºåˆ†æ•°ï¼Œä¼ å…¥ RM è¾“å‡ºçš„ `results`ï¼Œä¼ å‡ºæå–åçš„åˆ†æ•°ã€‚
5. å¦‚æœè¦åš rm å’Œè§„åˆ™çš„æ··åˆæ‰“åˆ†ï¼Œ`reward_model.combine_reward` è®¾ä¸º Trueï¼Œç„¶ååœ¨ `verl/verl/utils/reward_score` ä¸­å®šä¹‰è§„åˆ™ reward çš„è®¡ç®—æ–¹æ³•ï¼ˆè¿™å—å„¿å†™çš„æœ‰ç‚¹å„¿è„ã€‚ã€‚ã€‚ï¼‰ã€‚
6. ç„¶åå°±å¯ä»¥æ„‰å¿«åœ°ç”¨ rm è¾…åŠ© rl è®­ç»ƒå•¦ï½

## Demoï¼š
ä»¥ç”µè¯éª‘æ‰‹è”ç³»ä¸åˆ°ç”¨æˆ·åœºæ™¯ä¸‹çŸ¥è¯†ä¸æ•°æ®èåˆæ¨¡å‹è®­ç»ƒä¸ºä¾‹ï¼Œpolicy model ä½¿ç”¨ @æ™ºæ³‰å“¥ æä¾›çš„ sft å†·å¯åŠ¨ longcat 18B æ¨¡å‹ï¼Œreward model ä½¿ç”¨ off-the-shelf qwq 32B æ¨¡å‹ã€‚è®­ç»ƒæ•°æ®ä¸º @æ™ºæ³‰å“¥ æä¾›çš„ä¸šåŠ¡æ•°æ®ï¼Œrm æ‰“åˆ†è§„åˆ™ä¸ºä¸ @æ™¯æ™´å§ æ‹Ÿå®šçš„ä¸‰ç¯èŠ‚åˆ†ç»´åº¦æ‰“åˆ†æ¨¡å¼ã€‚
æ•°æ®å¤„ç†è„šæœ¬ä½¿ç”¨ `verl/examples/data_preprocess/process_rider.py`
reward_function ä½¿ç”¨ `rider_reward`

### å•æœºå¤šå¡è¿è¡Œè„šæœ¬ï¼š
```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
cd ..
THE_HOME=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/zhangrunlai
MODEL_PATH=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/checkpoints/llama_factory/longcat_18b_kefu_base_sft_training_train_data_v2_4_lr-1e-5_epoch-5
# MODEL_PATH=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/ruanjingqing/program/llm/llm/Qwen2.5-7B-Instruct
# REWARD_MODEL_PATH=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/ruanjingqing/program/llm/llm/Qwen2.5-32B-Instruct
REWARD_MODEL_PATH1=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/checkpoints/llama_factory/longcat_18b_kefu_base_sft_training_train_data_v2_4_lr-1e-5_epoch-5
REWARD_MODEL_PATH2=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/checkpoints/llama_factory/longcat_18b_kefu_base_sft_training_train_data_v2_4_lr-1e-5_epoch-5
TRAIN_FILE=$THE_HOME/data/rider_v4/train_2.parquet
REWARD_FUCTION1=rider_reward
REWARD_FUCTION2=consistency_reward

python3 verl/utils/bge_sim.py &
echo "bge_sim.py started"

PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \
 algorithm.adv_estimator=reinforce_plus_plus \
 data.train_files=$TRAIN_FILE \
 data.val_files=$TRAIN_FILE \
 data.train_batch_size=32 \
 data.max_prompt_length=6144 \
 data.max_response_length=2048 \
 data.filter_overlong_prompts=True \
 reward_model.enable=True \
 reward_model.reward_function=[${REWARD_FUCTION1},${REWARD_FUCTION2}] \
 reward_model.model.path=[${REWARD_MODEL_PATH1},${REWARD_MODEL_PATH2}] \
 reward_model.micro_batch_size=4 \
 reward_model.reward_vllm_rollout.temperature=[0.7,0.7] \
 reward_model.reward_combine_func=['add','add'] \
 reward_model.reward_model_type=grm \
 reward_model.model.fsdp_config.param_offload=True \
 reward_model.model.input_tokenizer=$MODEL_PATH \
 reward_model.reward_vllm_rollout.gpu_memory_utilization=[0.8,0.8] \
 reward_model.reward_weight=[0.3,0.5] \
 reward_model.model.tensor_model_parallel_size=4 \
 reward_model.reward_vllm_rollout.tensor_model_parallel_size=4 \
 actor_rollout_ref.model.path=$MODEL_PATH \
 actor_rollout_ref.actor.optim.lr=1e-6 \
 actor_rollout_ref.actor.ppo_mini_batch_size=8 \
 actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 \
 actor_rollout_ref.rollout.tensor_model_parallel_size=4 \
 actor_rollout_ref.actor.fsdp_config.param_offload=True \
 actor_rollout_ref.actor.fsdp_config.optimizer_offload=True \
 actor_rollout_ref.rollout.gpu_memory_utilization=0.6 \
 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \
 actor_rollout_ref.rollout.n=4 \
 actor_rollout_ref.actor.use_dynamic_bsz=True \
 algorithm.kl_ctrl.kl_coef=0.001 \
 trainer.project_name='0705_grpo_rider_debug' \
 trainer.logger=['console'] \
 trainer.val_before_train=False \
 trainer.default_hdfs_dir=null \
 trainer.n_gpus_per_node=8 \
 trainer.nnodes=1 \
 trainer.save_freq=30 \
 trainer.test_freq=1000000 \
 trainer.total_epochs=15 2>&1 | tee verl_demo.log

# actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8 \
#  critic.optim.lr=1e-5 \
#  critic.model.path=$MODEL_PATH \
#  critic.ppo_micro_batch_size_per_gpu=4 \
#  reward_model.enable=True \
#  reward_model.model.path=$REWARD_MODEL_PATH \
#  reward_model.micro_batch_size=4 \
```

### å¤šæœºå¤šå¡è¿è¡Œè„šæœ¬ï¼Œå‚è€ƒè‡ª @æ™¯æ™´å§ çš„è„šæœ¬ï¼š
```bash
#!/bin/bash
set -x
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

cur_work_dir=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/zhangrunlai/verl
cd $cur_work_dir
source $cur_work_dir/utils/export_env_verl.sh
export PATH=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/zhangrunlai/envs/myverl-3/bin:$PATH


# é»˜è®¤å€¼
NUM_EPISODES=20
LR_ACTOR=1e-6
n_samples_per_prompt=4
DATADIR=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/zhangrunlai/data/rider_v4/train_2.parquet
MODELDIR=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/checkpoints/llama_factory/longcat_18b_kefu_base_sft_training_train_data_v2_4_lr-1e-5_epoch-5
# REWARD_MODEL_PATH=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/models/longcat_18b_V1_10_sanqing
WARM=no
experiment_name=zrl_grm_longcat_w03_add-add
is_custome_model=False
VAL_DATADIR=""

THE_HOME=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/zhangrunlai

DATAPATH=$DATADIR
VALDATAPATH=$DATADIR

PRETRAIN_DIR=$MODELDIR
MODEL_SAVE_NAME=${experiment_name}_episodes_${NUM_EPISODES}_actor_lr_${LR_ACTOR}_nsamples_${n_samples_per_prompt}
SAVE_DIR=$WORKDIR/results/checkpoint/rl/longcat_18b_V1_10_sanqing/$MODEL_SAVE_NAME
# REWARD_FUCTION1=rider_reward

REWARD_MODEL_PATH1=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/checkpoints/llama_factory/longcat_18b_kefu_base_sft_training_train_data_v2_4_lr-1e-5_epoch-5
REWARD_MODEL_PATH2=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/checkpoints/llama_factory/longcat_18b_kefu_base_sft_training_train_data_v2_4_lr-1e-5_epoch-5
TRAIN_FILE=$THE_HOME/data/rider_v4/train_2.parquet
REWARD_FUCTION1=rider_reward
REWARD_FUCTION2=consistency_reward

python3 verl/utils/bge_sim.py &
echo "bge_sim.py started"

echo "save dir:" $SAVE_DIR

# export WANDB_DIR=${SAVE_DIR}/wandb
# mkdir -p $WANDB_DIR

# export RAY_STORAGE=$WANDB_DIR/ray_storage
# mkdir -p $RAY_STORAGE



if [ "$NODE_RANK" -eq 0 ]; then
   # åœ¨å®¹å™¨ä¸­å¯åŠ¨ Ray çš„ä¸»èŠ‚ç‚¹
   # ray start --head --node-ip-address $MASTER_ADDR --port=6379 \
   # --num-gpus $GPUS_PER_NODE \
   # --temp-dir $WANDB_DIR\
   # --storage $RAY_STORAGE \
   #--include-dashboard=false
   ray start --head --node-ip-address $MASTER_ADDR --port=6379 --num-gpus $GPUS_PER_NODE
   # python3 -u $WORKDIR/utils/ray_start.py &
   sleep 30
   # è®°å½•å½“å‰ç›®å½•
   CUR_DIR=$(pwd)
   echo $CUR_DIR
   echo $PATH
   ray job submit --runtime-env-json="{\"working_dir\": \"$CUR_DIR\",
                           \"env_vars\": {\"PATH\": \"$PATH\"}, 
                           \"excludes\": [
                              \"afo-base-0.0.1-SNAPSHOT.jar\", 
                              \"jdk-11.0.12/lib/modules\", 
                              \"jdk-11.0.12/lib/src.zip\", 
                              \"jdk-11.0.12/lib/server/libjvm.so\", 
                              \"jdk-11.0.12/jmods/java.base.jmod\", 
                              \"jdk-11.0.12/jmods/java.desktop.jmod\",
                              \"data/*\",
                              \"results/*\",
                              \"hope.code.tar.gz\",
                              \"afo-dist-user-files.tar.gz\",
                              \"libbwfs76.so\"
                           ]}" \
    -- python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=reinforce_plus_plus \
    data.train_files=$DATAPATH \
    data.val_files=$VALDATAPATH \
    data.train_batch_size=128 \
    data.max_prompt_length=6144 \
    data.max_response_length=2048 \
    data.filter_overlong_prompts=True \
    data.truncation='error' \
    reward_model.enable=True \
    reward_model.reward_function=[${REWARD_FUCTION1},${REWARD_FUCTION2}] \
    reward_model.model.path=[${REWARD_MODEL_PATH1},${REWARD_MODEL_PATH2}] \
    reward_model.micro_batch_size=4 \
    reward_model.reward_vllm_rollout.temperature=[0.7,0.7] \
    reward_model.reward_combine_func=['add','add'] \
    reward_model.reward_model_type=grm \
    reward_model.model.fsdp_config.param_offload=True \
    reward_model.model.input_tokenizer=$PRETRAIN_DIR \
    reward_model.reward_vllm_rollout.gpu_memory_utilization=[0.8,0.8] \
    reward_model.reward_weight=[0.3,0.5] \
    reward_model.model.tensor_model_parallel_size=4 \
    reward_model.reward_vllm_rollout.tensor_model_parallel_size=4 \
    actor_rollout_ref.model.path=$PRETRAIN_DIR\
    actor_rollout_ref.actor.optim.lr=$LR_ACTOR \
    actor_rollout_ref.actor.use_dynamic_bsz=True\
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.actor.ppo_mini_batch_size=64 \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.0001 \
    actor_rollout_ref.actor.kl_loss_type=low_var_kl \
    actor_rollout_ref.actor.entropy_coeff=0 \
    actor_rollout_ref.actor.fsdp_config.param_offload=True \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=True \
    actor_rollout_ref.rollout.tensor_model_parallel_size=4 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.n=$n_samples_per_prompt \
    actor_rollout_ref.rollout.temperature=0.8 \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.8 \
    actor_rollout_ref.ref.fsdp_config.param_offload=True \
    trainer.logger=['console'] \
    trainer.project_name='RL-GRM-Longcat' \
    trainer.val_before_train=False \
    trainer.experiment_name=$experiment_name \
    trainer.n_gpus_per_node=$GPUS_PER_NODE \
    trainer.nnodes=$NNODES \
    trainer.default_local_dir=$SAVE_DIR \
    trainer.default_hdfs_dir=null \
    trainer.save_freq=30 \
    trainer.test_freq=1000000 \
    trainer.total_epochs=$NUM_EPISODES
else
   echo "WORKER NODE"
   # python3 -u $WORKDIR/utils/ray_start.py &
   sleep 10
   # åœ¨æ›´å¤šèŠ‚ç‚¹ä¸Šå¯åŠ¨ Ray
   RAY_START_CMD="ray start --block --address=${MASTER_ADDR}:6379 --num-gpus ${GPUS_PER_NODE}"
   # RAY_START_CMD="ray start --block --address=${MASTER_ADDR}:6379 \
   # --num-gpus ${GPUS_PER_NODE} \
   # --temp-dir $WANDB_DIR \
   # --storage $RAY_STORAGE"
   echo $RAY_START_CMD
   $RAY_START_CMD
fi

```

## TODO Listï¼š
1. GRM vllm æ‰“åˆ†æ”¯æŒã€‚â˜‘
2. BT / critique + BT çš„ vllm æ‰“åˆ†æ”¯æŒã€‚ â–¡
3. RM sglang åç«¯æ”¯æŒã€‚ â–¡
4. æ€§èƒ½ä¼˜åŒ–ï¼šæ¨¡å‹å¸è½½æ˜¾å­˜ç¢ç‰‡  â–¡
5. æ€§èƒ½ä¼˜åŒ–ï¼špolicy å’Œ reward tp_size æ†ç»‘çš„æ€§èƒ½é™åˆ¶ â–¡
6. å¤š rm å®ä¾‹ååŒæ‰“åˆ†ã€‚â˜‘
7. ...