<h1 style="text-align: center;">VeRL-with-RMs: ‰ΩøÁî®Â•ñÂä±Ê®°ÂûãÊõ¥Â•ΩÂú∞ÊåáÂØº Policy ËÆ≠ÁªÉ </h1>

üåü ÂÖ≥ÈîÆÁâπÊÄßÔºö
ÊîØÊåÅÂ§öÁßçÂ•ñÂä±Ê®°ÂûãÔºàRMÔºâÁöÑÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ„ÄÇÂü∫‰∫é vllm ÂêéÁ´ØÂÆûÁé∞È´òÊïàÁöÑ RM ÊâìÂàÜÔºåÂêåÊó∂ÈÄöËøáÂä®ÊÄÅË£ÖÂç∏ËΩΩ RM ÂèäÂêÑÁªÑ‰ª∂Ôºå‰ºòÂåñÊòæÂ≠òÂà©Áî®„ÄÇÊîØÊåÅ RM ‰∏éËßÑÂàôÁöÑÊ∑∑ÂêàÊâìÂàÜÊ®°Âºè„ÄÇÊîØÊåÅÂ§ö RM ÂÆû‰æãÂçèÂêåÊâìÂàÜÊ®°Âºè„ÄÇ


## ‰ΩøÁî®ÊñπÊ≥ïÔºàÊöÇÊîØÊåÅÂà∞GRMÔºâÔºö

1. Âú®Ê†áÂáÜÁöÑÂü∫‰∫éËßÑÂàôÁöÑ RL ËÆ≠ÁªÉÊµÅÁ®ãÊâÄÈúÄË¶ÅÂáÜÂ§áÁöÑÁªÑ‰ª∂Âü∫Á°Ä‰∏äÔºåÂáÜÂ§á‰∏Ä‰∏™ RM Êàñ off-the-shelf LLM ‰Ωú‰∏∫ÊâìÂàÜÊ®°Âûã„ÄÇ
2. ÂèÇËÄÉ `verl/verl/trainer/config/ppo_trainer.yaml` ‰∏≠ `reward_model` È°π‰∏ãÂÆö‰πâÁöÑÁõ∏ÂÖ≥ÂèÇÊï∞ÔºåÂú®ËøêË°å `verl.trainer.main_ppo` ÈÄöËøáÂëΩ‰ª§Ë°åÂèÇÊï∞‰º†ÂÖ•‰ª•ÊéßÂà∂Ê®°ÂûãË°å‰∏∫„ÄÇÊØîÂ¶Ç `reward_model.model.path` ÊéßÂà∂ËØªÂèñÁöÑ RM ÁöÑË∑ØÂæÑÔºå`reward_model.reward_model_type` ÊéßÂà∂ RM ÊâìÂàÜÁ±ªÂûãÁ≠âÁ≠â„ÄÇ
3. Êï∞ÊçÆÂ§ÑÁêÜÊ†ºÂºèË¶ÅÊ±ÇÔºöÈúÄË¶ÅÂú®Ê®°ÂûãËØªÂèñÁöÑ parquet Êñá‰ª∂‰∏≠ÂÆö‰πâÂ¶Ç‰∏ãÊï∞ÊçÆÈ°πÔºå`prompt`Ôºàpolicy model ÁöÑ querysÔºâÔºå`rm_prompt`Ôºàreward model ÁöÑ promptsÔºå‰πüÂèØ‰ª•ÊòØÁ©∫strÔºåÂú®ÂêéÁª≠ `reward_function` ‰∏≠ÂÜçÂÆö‰πâÔºâÔºå`reward_model`ÔºàÂèÇËÄÉÁ≠îÊ°àÔºâ„ÄÇÊï∞ÊçÆÂ§ÑÁêÜÊñá‰ª∂ÂèØÂèÇËÄÉ `verl/examples/data_preprocess/process_rider.py`„ÄÇ
4. Reward ÂâçÂêéÂ§ÑÁêÜÔºö

   (1) Âú® `verl/workers/reward_function` Êñá‰ª∂Â§π‰∏ãÂÆö‰πâÁî®‰∫é Reward ËÆ°ÁÆóÂâçÂ§ÑÁêÜ‰∏éÂêéÂ§ÑÁêÜÊñπÊ≥ïÁöÑ `.py` Êñá‰ª∂ÔºåÂπ∂‰∏îÈúÄË¶ÅÁ°Æ‰øù config ÁöÑ `reward_model.reward_function` ÁöÑÂÄº‰∏éÊñá‰ª∂ÂêçÁõ∏Âêå‰ªéËÄå‰ΩøÊñπÊ≥ïËÉΩÂ§üÊ≠£Á°ÆÊ≥®ÂÜå„ÄÇÊØîÂ¶ÇÔºåÂú® `rider_reward.py` ÈáåÂÆö‰πâ‰∫ÜÁî®‰∫éÈ™ëÊâã RL ËÆ≠ÁªÉÁöÑ reward Â§ÑÁêÜËßÑÂàôÔºåÂ∞±Ë¶ÅÂú® config ÊàñÂëΩ‰ª§Ë°å‰º†ÂèÇ‰∏≠Â∞Ü `reward_model.reward_function` ÁöÑÂÄºËµã‰∏∫ `rider_reward`„ÄÇ
   
   (2) ÊØè‰∏™ `reward_function` Êñá‰ª∂‰∏≠ÈÉΩÈúÄË¶ÅÂÆûÁé∞‰∏§‰∏™ÂáΩÊï∞ `rm_preprocess()` ‰ª•Âèä `rm_postprocess()`Ôºå`rm_preprocess()` Ë¥üË¥£Âà∂‰Ωú RM ÁöÑËæìÂÖ•ÔºåÈúÄ‰º†ÂÖ• `text_prompts`ÔºàRM ÁöÑpromptÔºâÔºå`text_responses`ÔºàPolicy ÁöÑrolloutÊñáÊú¨ÔºâÔºå`tgt_tokenizer`ÔºàRM ÁöÑ tokenizerÔºâÔºå‰º†Âá∫Ë¶ÅËæìÂÖ•Áªô RM ÁöÑÊñáÊú¨„ÄÇ`rm_postprocess()` ÈúÄË¶ÅÂØπ RM ËæìÂá∫ÂÜÖÂÆπËøõË°åÂêéÂ§ÑÁêÜÊèêÂèñÂá∫ÂàÜÊï∞Ôºå‰º†ÂÖ• RM ËæìÂá∫ÁöÑ `results`Ôºå‰º†Âá∫ÊèêÂèñÂêéÁöÑÂàÜÊï∞„ÄÇ
5. Â¶ÇÊûúË¶ÅÂÅö rm ÂíåËßÑÂàôÁöÑÊ∑∑ÂêàÊâìÂàÜÔºå`reward_model.combine_reward` ËÆæ‰∏∫ TrueÔºåÁÑ∂ÂêéÂú® `verl/verl/utils/reward_score` ‰∏≠ÂÆö‰πâËßÑÂàô reward ÁöÑËÆ°ÁÆóÊñπÊ≥ïÔºàËøôÂùóÂÑøÂÜôÁöÑÊúâÁÇπÂÑøËÑè„ÄÇ„ÄÇ„ÄÇÔºâ„ÄÇ
6. ÁÑ∂ÂêéÂ∞±ÂèØ‰ª•ÊÑâÂø´Âú∞Áî® rm ËæÖÂä© rl ËÆ≠ÁªÉÂï¶ÔΩû

## DemoÔºö
‰ª•ÁîµËØùÈ™ëÊâãËÅîÁ≥ª‰∏çÂà∞Áî®Êà∑Âú∫ÊôØ‰∏ãÁü•ËØÜ‰∏éÊï∞ÊçÆËûçÂêàÊ®°ÂûãËÆ≠ÁªÉ‰∏∫‰æãÔºåpolicy model ‰ΩøÁî® @Êô∫Ê≥âÂì• Êèê‰æõÁöÑ sft ÂÜ∑ÂêØÂä® longcat 18B Ê®°ÂûãÔºåreward model ‰ΩøÁî® off-the-shelf qwq 32B Ê®°Âûã„ÄÇËÆ≠ÁªÉÊï∞ÊçÆ‰∏∫ @Êô∫Ê≥âÂì• Êèê‰æõÁöÑ‰∏öÂä°Êï∞ÊçÆÔºårm ÊâìÂàÜËßÑÂàô‰∏∫‰∏é @ÊôØÊô¥Âßê ÊãüÂÆöÁöÑ‰∏âÁéØËäÇÂàÜÁª¥Â∫¶ÊâìÂàÜÊ®°Âºè„ÄÇ
Êï∞ÊçÆÂ§ÑÁêÜËÑöÊú¨‰ΩøÁî® `verl/examples/data_preprocess/process_rider.py`
reward_function ‰ΩøÁî® `rider_reward`

### ÂçïÊú∫Â§öÂç°ËøêË°åËÑöÊú¨Ôºö
```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
cd ..
THE_HOME=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/zhangrunlai
MODEL_PATH=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/checkpoints/llama_factory/longcat_18b_kefu_base_sft_training_train_data_v2_4_lr-1e-5_epoch-5
# MODEL_PATH=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/ruanjingqing/program/llm/llm/Qwen2.5-7B-Instruct
# REWARD_MODEL_PATH=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/ruanjingqing/program/llm/llm/Qwen2.5-32B-Instruct
REWARD_MODEL_PATH1=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/checkpoints/llama_factory/longcat_18b_kefu_base_sft_training_train_data_v2_4_lr-1e-5_epoch-5
REWARD_MODEL_PATH2=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/checkpoints/llama_factory/longcat_18b_kefu_base_sft_training_train_data_v2_4_lr-1e-5_epoch-5
TRAIN_FILE=$THE_HOME/data/rider_v4/train_2.parquet
REWARD_FUCTION1=rider_reward
REWARD_FUCTION2=consistency_reward

python3 verl/utils/bge_sim.py &
echo "bge_sim.py started"

PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \
 algorithm.adv_estimator=reinforce_plus_plus \
 data.train_files=$TRAIN_FILE \
 data.val_files=$TRAIN_FILE \
 data.train_batch_size=32 \
 data.max_prompt_length=6144 \
 data.max_response_length=2048 \
 data.filter_overlong_prompts=True \
 reward_model.enable=True \
 reward_model.reward_function=[${REWARD_FUCTION1},${REWARD_FUCTION2}] \
 reward_model.model.path=[${REWARD_MODEL_PATH1},${REWARD_MODEL_PATH2}] \
 reward_model.micro_batch_size=4 \
 reward_model.reward_vllm_rollout.temperature=[0.7,0.7] \
 reward_model.reward_combine_func=['add','add'] \
 reward_model.reward_model_type=grm \
 reward_model.model.fsdp_config.param_offload=True \
 reward_model.model.input_tokenizer=$MODEL_PATH \
 reward_model.reward_vllm_rollout.gpu_memory_utilization=[0.8,0.8] \
 reward_model.reward_weight=[0.3,0.5] \
 reward_model.model.tensor_model_parallel_size=4 \
 reward_model.reward_vllm_rollout.tensor_model_parallel_size=4 \
 actor_rollout_ref.model.path=$MODEL_PATH \
 actor_rollout_ref.actor.optim.lr=1e-6 \
 actor_rollout_ref.actor.ppo_mini_batch_size=8 \
 actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 \
 actor_rollout_ref.rollout.tensor_model_parallel_size=4 \
 actor_rollout_ref.actor.fsdp_config.param_offload=True \
 actor_rollout_ref.actor.fsdp_config.optimizer_offload=True \
 actor_rollout_ref.rollout.gpu_memory_utilization=0.6 \
 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \
 actor_rollout_ref.rollout.n=4 \
 actor_rollout_ref.actor.use_dynamic_bsz=True \
 algorithm.kl_ctrl.kl_coef=0.001 \
 trainer.project_name='0705_grpo_rider_debug' \
 trainer.logger=['console'] \
 trainer.val_before_train=False \
 trainer.default_hdfs_dir=null \
 trainer.n_gpus_per_node=8 \
 trainer.nnodes=1 \
 trainer.save_freq=30 \
 trainer.test_freq=1000000 \
 trainer.total_epochs=15 2>&1 | tee verl_demo.log

# actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8 \
#  critic.optim.lr=1e-5 \
#  critic.model.path=$MODEL_PATH \
#  critic.ppo_micro_batch_size_per_gpu=4 \
#  reward_model.enable=True \
#  reward_model.model.path=$REWARD_MODEL_PATH \
#  reward_model.micro_batch_size=4 \
```

### Â§öÊú∫Â§öÂç°ËøêË°åËÑöÊú¨ÔºåÂèÇËÄÉËá™ @ÊôØÊô¥Âßê ÁöÑËÑöÊú¨Ôºö
```bash
#!/bin/bash
set -x
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

cur_work_dir=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/zhangrunlai/verl
cd $cur_work_dir
source $cur_work_dir/utils/export_env_verl.sh
export PATH=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/zhangrunlai/envs/myverl-3/bin:$PATH


# ÈªòËÆ§ÂÄº
NUM_EPISODES=20
LR_ACTOR=1e-6
n_samples_per_prompt=4
DATADIR=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/zhangrunlai/data/rider_v4/train_2.parquet
MODELDIR=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/checkpoints/llama_factory/longcat_18b_kefu_base_sft_training_train_data_v2_4_lr-1e-5_epoch-5
# REWARD_MODEL_PATH=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/models/longcat_18b_V1_10_sanqing
WARM=no
experiment_name=zrl_grm_longcat_w03_add-add
is_custome_model=False
VAL_DATADIR=""

THE_HOME=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/zhangrunlai

DATAPATH=$DATADIR
VALDATAPATH=$DATADIR

PRETRAIN_DIR=$MODELDIR
MODEL_SAVE_NAME=${experiment_name}_episodes_${NUM_EPISODES}_actor_lr_${LR_ACTOR}_nsamples_${n_samples_per_prompt}
SAVE_DIR=$WORKDIR/results/checkpoint/rl/longcat_18b_V1_10_sanqing/$MODEL_SAVE_NAME
# REWARD_FUCTION1=rider_reward

REWARD_MODEL_PATH1=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/checkpoints/llama_factory/longcat_18b_kefu_base_sft_training_train_data_v2_4_lr-1e-5_epoch-5
REWARD_MODEL_PATH2=/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai/users/caozhiquan/checkpoints/llama_factory/longcat_18b_kefu_base_sft_training_train_data_v2_4_lr-1e-5_epoch-5
TRAIN_FILE=$THE_HOME/data/rider_v4/train_2.parquet
REWARD_FUCTION1=rider_reward
REWARD_FUCTION2=consistency_reward

python3 verl/utils/bge_sim.py &
echo "bge_sim.py started"

echo "save dir:" $SAVE_DIR

# export WANDB_DIR=${SAVE_DIR}/wandb
# mkdir -p $WANDB_DIR

# export RAY_STORAGE=$WANDB_DIR/ray_storage
# mkdir -p $RAY_STORAGE



if [ "$NODE_RANK" -eq 0 ]; then
   # Âú®ÂÆπÂô®‰∏≠ÂêØÂä® Ray ÁöÑ‰∏ªËäÇÁÇπ
   # ray start --head --node-ip-address $MASTER_ADDR --port=6379 \
   # --num-gpus $GPUS_PER_NODE \
   # --temp-dir $WANDB_DIR\
   # --storage $RAY_STORAGE \
   #--include-dashboard=false
   ray start --head --node-ip-address $MASTER_ADDR --port=6379 --num-gpus $GPUS_PER_NODE
   # python3 -u $WORKDIR/utils/ray_start.py &
   sleep 30
   # ËÆ∞ÂΩïÂΩìÂâçÁõÆÂΩï
   CUR_DIR=$(pwd)
   echo $CUR_DIR
   echo $PATH
   ray job submit --runtime-env-json="{\"working_dir\": \"$CUR_DIR\",
                           \"env_vars\": {\"PATH\": \"$PATH\"}, 
                           \"excludes\": [
                              \"afo-base-0.0.1-SNAPSHOT.jar\", 
                              \"jdk-11.0.12/lib/modules\", 
                              \"jdk-11.0.12/lib/src.zip\", 
                              \"jdk-11.0.12/lib/server/libjvm.so\", 
                              \"jdk-11.0.12/jmods/java.base.jmod\", 
                              \"jdk-11.0.12/jmods/java.desktop.jmod\",
                              \"data/*\",
                              \"results/*\",
                              \"hope.code.tar.gz\",
                              \"afo-dist-user-files.tar.gz\",
                              \"libbwfs76.so\"
                           ]}" \
    -- python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=reinforce_plus_plus \
    data.train_files=$DATAPATH \
    data.val_files=$VALDATAPATH \
    data.train_batch_size=128 \
    data.max_prompt_length=6144 \
    data.max_response_length=2048 \
    data.filter_overlong_prompts=True \
    data.truncation='error' \
    reward_model.enable=True \
    reward_model.reward_function=[${REWARD_FUCTION1},${REWARD_FUCTION2}] \
    reward_model.model.path=[${REWARD_MODEL_PATH1},${REWARD_MODEL_PATH2}] \
    reward_model.micro_batch_size=4 \
    reward_model.reward_vllm_rollout.temperature=[0.7,0.7] \
    reward_model.reward_combine_func=['add','add'] \
    reward_model.reward_model_type=grm \
    reward_model.model.fsdp_config.param_offload=True \
    reward_model.model.input_tokenizer=$PRETRAIN_DIR \
    reward_model.reward_vllm_rollout.gpu_memory_utilization=[0.8,0.8] \
    reward_model.reward_weight=[0.3,0.5] \
    reward_model.model.tensor_model_parallel_size=4 \
    reward_model.reward_vllm_rollout.tensor_model_parallel_size=4 \
    actor_rollout_ref.model.path=$PRETRAIN_DIR\
    actor_rollout_ref.actor.optim.lr=$LR_ACTOR \
    actor_rollout_ref.actor.use_dynamic_bsz=True\
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.actor.ppo_mini_batch_size=64 \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.0001 \
    actor_rollout_ref.actor.kl_loss_type=low_var_kl \
    actor_rollout_ref.actor.entropy_coeff=0 \
    actor_rollout_ref.actor.fsdp_config.param_offload=True \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=True \
    actor_rollout_ref.rollout.tensor_model_parallel_size=4 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.n=$n_samples_per_prompt \
    actor_rollout_ref.rollout.temperature=0.8 \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.8 \
    actor_rollout_ref.ref.fsdp_config.param_offload=True \
    trainer.logger=['console'] \
    trainer.project_name='RL-GRM-Longcat' \
    trainer.val_before_train=False \
    trainer.experiment_name=$experiment_name \
    trainer.n_gpus_per_node=$GPUS_PER_NODE \
    trainer.nnodes=$NNODES \
    trainer.default_local_dir=$SAVE_DIR \
    trainer.default_hdfs_dir=null \
    trainer.save_freq=30 \
    trainer.test_freq=1000000 \
    trainer.total_epochs=$NUM_EPISODES
else
   echo "WORKER NODE"
   # python3 -u $WORKDIR/utils/ray_start.py &
   sleep 10
   # Âú®Êõ¥Â§öËäÇÁÇπ‰∏äÂêØÂä® Ray
   RAY_START_CMD="ray start --block --address=${MASTER_ADDR}:6379 --num-gpus ${GPUS_PER_NODE}"
   # RAY_START_CMD="ray start --block --address=${MASTER_ADDR}:6379 \
   # --num-gpus ${GPUS_PER_NODE} \
   # --temp-dir $WANDB_DIR \
   # --storage $RAY_STORAGE"
   echo $RAY_START_CMD
   $RAY_START_CMD
fi

```

## TODO ListÔºö
1. GRM vllm ÊâìÂàÜÊîØÊåÅ„ÄÇ‚òë
2. BT / critique + BT ÁöÑ vllm ÊâìÂàÜÊîØÊåÅ„ÄÇ ‚ñ°
3. RM sglang ÂêéÁ´ØÊîØÊåÅ„ÄÇ ‚ñ°
4. ÊÄßËÉΩ‰ºòÂåñÔºöÊ®°ÂûãÂç∏ËΩΩÊòæÂ≠òÁ¢éÁâá  ‚ñ°
5. ÊÄßËÉΩ‰ºòÂåñÔºöpolicy Âíå reward tp_size ÊçÜÁªëÁöÑÊÄßËÉΩÈôêÂà∂ ‚ñ°
6. Â§ö rm ÂÆû‰æãÂçèÂêåÊâìÂàÜ„ÄÇ‚òë
7. ...